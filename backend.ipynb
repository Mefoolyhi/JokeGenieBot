{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mefoolyhi/JokeGenieBot/blob/main/backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxELmob9QQ-L",
        "outputId": "203374a6-dc09-48d4-d382-8696694e0ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.10/dist-packages (13.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot) (2024.2.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot) (6.3.3)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot) (3.6.3)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot) (2023.4)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot) (4.2.2)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (1.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-telegram-bot requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-telegram-bot==13.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6eKqZ4BQUf9",
        "outputId": "dfb9e3a6-cad1-41a7-ddbf-0e2ecd8c8ad5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-telegram-bot==13.7\n",
            "  Downloading python_telegram_bot-13.7-py3-none-any.whl (490 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m490.1/490.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.7) (2024.2.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.7) (6.3.3)\n",
            "Collecting APScheduler==3.6.3 (from python-telegram-bot==13.7)\n",
            "  Downloading APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.7) (2023.4)\n",
            "Collecting cachetools==4.2.2 (from python-telegram-bot==13.7)\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (1.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.7) (5.2)\n",
            "Installing collected packages: cachetools, APScheduler, python-telegram-bot\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.3\n",
            "    Uninstalling cachetools-5.3.3:\n",
            "      Successfully uninstalled cachetools-5.3.3\n",
            "Successfully installed APScheduler-3.6.3 cachetools-4.2.2 python-telegram-bot-13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "IImUAUN82TeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc795754-7924-4d94-f64b-75afc6d51fbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCXr8tMj2m_h",
        "outputId": "38071da7-b84f-4262-cd69-b39750eb7576"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import os\n",
        "from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup\n",
        "from telegram.ext import Updater, CommandHandler, CallbackQueryHandler, CallbackContext\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import NLTKWordTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Ğ¢Ğ¾ĞºĞµĞ½ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ±Ğ¾Ñ‚Ğ°\n",
        "TOKEN = '7065562212:AAHiu-AYQ9ggAz9f0jlJoGNu_vM4FQPzDpQ'\n",
        "\n",
        "# Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ğ°Ğ¼Ğ¸\n",
        "JOKES_FILE = 'total_data_jokes_512.csv'\n",
        "# Ğ¤Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹\n",
        "HISTORY_FILE = 'user_favorites.csv'\n",
        "\n",
        "# Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸Ñ…ÑÑ, Ğ½Ğµ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ\n",
        "user_favorites = {}\n",
        "user_dislikes = {}\n",
        "user_viewed = {}\n",
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "encoder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ğ¾Ğ² Ğ¸Ğ· CSV-Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "def load_jokes(filename):\n",
        "    total_data = pd.read_csv('total_data_jokes_512.csv', encoding = \"utf-8\")\n",
        "    jokes = total_data['joke']\n",
        "    encoded_data = encoder.encode(jokes) # ÑÑ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ğ½ÑƒÑ‚ 10, ÑÑ‚Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾\n",
        "    index.add_with_ids(encoded_data, np.arange(len(jokes)))\n",
        "    return jokes\n",
        "\n",
        "# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ\n",
        "jokes = load_jokes(JOKES_FILE)\n",
        "\n",
        "# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· CSV-Ñ„Ğ°Ğ¹Ğ»Ğ°\n",
        "def load_user_favorites(filename):\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            next(reader)  # ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº\n",
        "            for row in reader:\n",
        "                if row:  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ½Ğµ Ğ¿ÑƒÑÑ‚Ğ°Ñ\n",
        "                    try:\n",
        "                        user_id = int(row[0])\n",
        "                        likes = row[1].split('|') if row[1] else []\n",
        "                        dislikes = row[2].split('|') if row[2] else []\n",
        "                        viewed = row[3].split('|') if row[3] else []\n",
        "                        user_favorites[user_id] = likes\n",
        "                        user_dislikes[user_id] = dislikes\n",
        "                        user_viewed[user_id] = viewed\n",
        "                    except ValueError:\n",
        "                        print(f\"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ user_id Ğ² Ñ†ĞµĞ»Ğ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾: {row[0]}\")\n",
        "    else:\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['user_id', 'favorites', 'dislikes', 'viewed'])\n",
        "\n",
        "# Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² CSV-Ñ„Ğ°Ğ¹Ğ»\n",
        "def save_user_favorites(filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['user_id', 'favorites', 'dislikes', 'viewed'])\n",
        "        for user_id in user_favorites:\n",
        "            favorites = '|'.join(user_favorites.get(user_id, []))\n",
        "            dislikes = '|'.join(user_dislikes.get(user_id, []))\n",
        "            viewed = '|'.join(user_viewed.get(user_id, []))\n",
        "            writer.writerow([user_id, favorites, dislikes, viewed])\n",
        "\n",
        "# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ\n",
        "load_user_favorites(HISTORY_FILE)\n",
        "\n",
        "def start(update: Update, context: CallbackContext) -> None:\n",
        "    print(\"Received start command!\")  # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ \"start\"\n",
        "    user_id = update.message.from_user.id\n",
        "    if user_id not in user_favorites:\n",
        "        user_favorites[user_id] = []\n",
        "    if user_id not in user_dislikes:\n",
        "        user_dislikes[user_id] = []\n",
        "    if user_id not in user_viewed:\n",
        "        user_viewed[user_id] = []\n",
        "\n",
        "    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ğ¾Ğ²\n",
        "    context.user_data['viewed_jokes'] = []\n",
        "\n",
        "    keyboard = [\n",
        "        [InlineKeyboardButton(\"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚\", callback_data='show_joke')],\n",
        "        [InlineKeyboardButton(\"ĞŸĞ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸ĞµÑÑ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ñ‹\", callback_data='show_favorites')]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "    update.message.reply_text('ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞĞ°Ğ¶Ğ¼Ğ¸ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Ğ½Ğ¸Ğ¶Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸ĞµÑÑ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ñ‹.', reply_markup=reply_markup)\n",
        "\n",
        "def show_joke(update: Update, context: CallbackContext) -> None:\n",
        "    query = update.callback_query\n",
        "    query.answer()\n",
        "    user_id = query.from_user.id\n",
        "\n",
        "    if user_id not in user_favorites:\n",
        "        user_favorites[user_id] = []\n",
        "    if user_id not in user_dislikes:\n",
        "        user_dislikes[user_id] = []\n",
        "    if user_id not in user_viewed:\n",
        "        user_viewed[user_id] = []\n",
        "\n",
        "    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ 'viewed_jokes', ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑ‰Ğµ Ğ½Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½\n",
        "    if 'viewed_jokes' not in context.user_data:\n",
        "        context.user_data['viewed_jokes'] = []\n",
        "\n",
        "    viewed_jokes = context.user_data['viewed_jokes']\n",
        "\n",
        "    last_liked = user_favorites[user_id][-5:]\n",
        "\n",
        "\n",
        "    if last_liked:\n",
        "        query_vector = encoder.encode(last_liked)\n",
        "        top_k = index.search(query_vector, 10)\n",
        "        top_jokes = []\n",
        "        for _id in top_k[1][0]:\n",
        "            joke = jokes[_id]\n",
        "            if joke not in viewed_jokes:\n",
        "                top_jokes.append(joke)\n",
        "\n",
        "        unliked = user_dislikes[user_id]\n",
        "        if unliked and top_k:\n",
        "            tokenizer = NLTKWordTokenizer()\n",
        "            unliked_tok = [tokenizer.tokenize(x.lower()) for x in unliked]\n",
        "            top_jokes_tok = [tokenizer.tokenize(x.lower()) for x in top_jokes]\n",
        "\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "            russian_stopwords = stopwords.words(\"russian\")\n",
        "            unliked_tokens = []\n",
        "            for row in unliked_tok:\n",
        "                row_tokens = []\n",
        "                for token in row:\n",
        "                    if token not in russian_stopwords and token not in punctuation:\n",
        "                        row_tokens.append(lemmatizer.lemmatize(token))\n",
        "                unliked_tokens.append(' '.join(row_tokens))\n",
        "\n",
        "            top_tokens = []\n",
        "            for row in top_jokes_tok:\n",
        "                row_tokens = []\n",
        "                for token in row:\n",
        "                    if token not in russian_stopwords and token not in punctuation:\n",
        "                        row_tokens.append(lemmatizer.lemmatize(token))\n",
        "                top_tokens.append(row_tokens)\n",
        "\n",
        "\n",
        "            tf_idf = TfidfVectorizer()\n",
        "            unliked_array = tf_idf.fit_transform(unliked_tokens)\n",
        "            threshold = 0.2 # Ñ‚ÑƒÑ‚ ĞµĞ³Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°Ğ´Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚ÑŒ\n",
        "            important_words = [word for word, score in zip(tf_idf.get_feature_names_out(), unliked_array.toarray()[0]) if score >= threshold]\n",
        "            important_set = set(important_words)\n",
        "            counts = []\n",
        "            for joke in top_tokens:\n",
        "                local_count = 0\n",
        "                for token in joke:\n",
        "                    if token in important_words:\n",
        "                        local_count += 1\n",
        "                counts.append(local_count)\n",
        "\n",
        "            final_joke = top_jokes[np.argmin(counts)]\n",
        "        else:\n",
        "            final_joke = random.choice(top_jokes)\n",
        "    else:\n",
        "        available_jokes = [j for j in jokes if j not in viewed_jokes]\n",
        "\n",
        "        if not available_jokes:\n",
        "            query.message.reply_text(\"Ğ’ÑĞµ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ñ‹ ÑƒĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ñ‹!\")\n",
        "            return\n",
        "\n",
        "        final_joke = random.choice(available_jokes)\n",
        "\n",
        "\n",
        "    context.user_data['current_joke'] = final_joke\n",
        "    context.user_data['viewed_jokes'].append(final_joke)\n",
        "    user_viewed[user_id].append(final_joke)\n",
        "\n",
        "    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº\n",
        "    save_user_favorites(HISTORY_FILE)\n",
        "\n",
        "    keyboard = [\n",
        "        [InlineKeyboardButton(\"ğŸ‘ ĞŸĞ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ»ÑÑ\", callback_data='like'),\n",
        "         InlineKeyboardButton(\"ğŸ‘ ĞĞµ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ»ÑÑ\", callback_data='dislike')],\n",
        "        [InlineKeyboardButton(\"Ğ•Ñ‰Ğµ\", callback_data='show_joke')],\n",
        "        [InlineKeyboardButton(\"ĞŸĞ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸ĞµÑÑ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ñ‹\", callback_data='show_favorites')]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚\n",
        "    query.message.reply_text(text=final_joke, reply_markup=reply_markup)\n",
        "\n",
        "def show_favorites(update: Update, context: CallbackContext) -> None:\n",
        "    query = update.callback_query\n",
        "    query.answer()\n",
        "    user_id = query.from_user.id\n",
        "\n",
        "    if user_id not in user_favorites or not user_favorites[user_id]:\n",
        "        query.edit_message_text(text=\"Ğ£ Ğ²Ğ°Ñ Ğ½ĞµÑ‚ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ğ¾Ğ².\")\n",
        "        # Ñ‚ÑƒÑ‚ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ ĞºĞ½Ğ¾Ğ¿Ğ¾Ğº\n",
        "        return\n",
        "\n",
        "    favorites = user_favorites[user_id]\n",
        "    favorites_text = \"\\n\\n\".join(favorites)\n",
        "\n",
        "    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ğ¾Ğ²\n",
        "    query.edit_message_text(text=f\"Ğ’Ğ°ÑˆĞ¸ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸ĞµÑÑ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ñ‹:\\n\\n{favorites_text}\")\n",
        "\n",
        "    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ \"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚\"\n",
        "    keyboard = [\n",
        "        [InlineKeyboardButton(\"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚\", callback_data='show_joke')]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "    query.message.reply_text('ĞĞ°Ğ¶Ğ¼Ğ¸ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Ğ½Ğ¸Ğ¶Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚.', reply_markup=reply_markup)\n",
        "\n",
        "def handle_feedback(update: Update, context: CallbackContext) -> None:\n",
        "    query = update.callback_query\n",
        "    query.answer()\n",
        "    user_id = query.from_user.id\n",
        "\n",
        "    if user_id not in user_favorites:\n",
        "        user_favorites[user_id] = []\n",
        "    if user_id not in user_dislikes:\n",
        "        user_dislikes[user_id] = []\n",
        "    if user_id not in user_viewed:\n",
        "        user_viewed[user_id] = []\n",
        "\n",
        "    feedback = query.data\n",
        "    current_joke = context.user_data.get('current_joke', None)\n",
        "\n",
        "    if current_joke:\n",
        "        if feedback == 'like':\n",
        "            user_favorites[user_id].append(current_joke)\n",
        "        elif feedback == 'dislike':\n",
        "            user_dislikes[user_id].append(current_joke)\n",
        "\n",
        "        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº\n",
        "        save_user_favorites(HISTORY_FILE)\n",
        "\n",
        "    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²\n",
        "    query.message.reply_text(\"Ğ¡Ğ¿Ğ°ÑĞ¸Ğ±Ğ¾ Ğ·Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²!\")\n",
        "\n",
        "    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ \"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚\"\n",
        "    keyboard = [\n",
        "        [InlineKeyboardButton(\"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚\", callback_data='show_joke')],\n",
        "        [InlineKeyboardButton(\"ĞŸĞ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ²ÑˆĞ¸ĞµÑÑ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚Ñ‹\", callback_data='show_favorites')]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "    query.message.reply_text('ĞĞ°Ğ¶Ğ¼Ğ¸ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Ğ½Ğ¸Ğ¶Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ½ĞµĞºĞ´Ğ¾Ñ‚.', reply_markup=reply_markup)\n",
        "\n",
        "def main() -> None:\n",
        "    updater = Updater(TOKEN)\n",
        "    dispatcher = updater.dispatcher\n",
        "\n",
        "    dispatcher.add_handler(CommandHandler(\"start\", start))\n",
        "    dispatcher.add_handler(CallbackQueryHandler(show_joke, pattern='show_joke'))\n",
        "    dispatcher.add_handler(CallbackQueryHandler(handle_feedback, pattern='like|dislike'))\n",
        "    dispatcher.add_handler(CallbackQueryHandler(show_favorites, pattern='show_favorites'))\n",
        "\n",
        "    updater.start_polling()\n",
        "    updater.idle()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6by0WMhknJhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b2633f-fa60-4508-f3ca-1d437e4ed2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}